{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8VR8TxNxqk7"
      },
      "source": [
        "# Installation and Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJnIx8Q8xY6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4690890-30e9-48f1-feb4-e53b5af11f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# dependencies\n",
        "import re\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# file management\n",
        "drive.mount('/content/drive')\n",
        "WORK_DIR = '/content/drive/MyDrive/Projects/skillextraction'\n",
        "\n",
        "# work dir shortcut function\n",
        "def work_dir(*args):\n",
        "    return os.path.join(WORK_DIR, *args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTkVCN_oudy"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config container\n",
        "class C:\n",
        "\n",
        "    # architecture\n",
        "    BASE_MODEL = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "    SEQ_LENGTH = 256\n",
        "    IS_SKILL_DIM = 8\n",
        "    ATP_TEMPERATURE = 0.005\n",
        "\n",
        "    # training\n",
        "    N_LAYERS = 5\n",
        "    LR = 1e-6\n",
        "    LR_INITIAL = 1e-8\n",
        "    LR_LAYER_FACTOR = 0.5\n",
        "    LR_REDUCE_FACTOR = 0.1\n",
        "    LR_WARMUP_FACTOR = 1.0005\n",
        "    TRAIN_METHOD = 'mnr' # 'direct', 'mnr'\n",
        "    EPOCHS = 100\n",
        "    PER_EPOCH = 10 # training samples per epoch per skill\n",
        "    BATCH_SIZE = 96\n",
        "    PATIENCE = 3 # early stopping\n",
        "    IS_SKILL_WEIGHT = 0.25 # loss coefficient\n",
        "    IS_SKILL_FP_PENALTY = 0.0005 # false positives loss multipler\n",
        "    SKILL_ID_TEMP = 0.05 # loss temperature\n",
        "    SKILL_ID_TEMP_INITIAL = 1.0\n",
        "    SKILL_ID_TEMP_FACTOR = 0.9999\n",
        "\n",
        "    # regularization\n",
        "    DROPOUT_RATE = 0.1\n",
        "    WEIGHT_DECAY_RATE = 0.1\n",
        "\n",
        "    # system\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    NUM_WORKERS = 2\n",
        "    PREFETCH_FACTOR = 1\n",
        "\n",
        "    # export path\n",
        "    def PATH(postfix=''):\n",
        "        return work_dir('experiments', 'llm_data_model' + postfix)\n",
        "\n",
        "# check config-aggregated path\n",
        "C.PATH('.csv')"
      ],
      "metadata": {
        "id": "imF1lwZErTY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "5d997aac-cf9f-411d-d978-48272ce26248"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Projects/skillextraction/experiments/llm_data_model.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r531y35IvrYe"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get esco english/danish\n",
        "esco_en = pd.read_csv(work_dir('ESCO', 'ESCO dataset - v1.1.2 - classification - en - csv', 'skills_en.csv'))\n",
        "esco_da = pd.read_csv(work_dir('ESCO', 'ESCO dataset - v1.1.2 - classification - da - csv', 'skills_da.csv'))\n",
        "\n",
        "# get proxies (english labels)\n",
        "proxies = esco_en[['conceptUri', 'preferredLabel']].rename(columns={'preferredLabel': 'sentence'}).sort_values('conceptUri').reset_index(drop=True)\n",
        "\n",
        "# get real sentences\n",
        "reals = pd.concat([pd.read_csv(work_dir('Annotated_data', s))\n",
        "                   for s in os.listdir(work_dir('Annotated_data')) if re.match(r'^sentences\\_[0-9]+\\.csv$', s)])\n",
        "\n",
        "# filter real sentences conservatively in relation to llm instructions\n",
        "skills = reals[reals['conceptUri'].notna() & (reals['conceptUri'] != '') & (reals['is_skill'] == 1)].copy()\n",
        "nonskills = reals[(reals['conceptUri'].isna() | (reals['conceptUri'] == '')) & (reals['is_skill'] == 0)].copy()\n",
        "\n",
        "# sanity\n",
        "nonskills['conceptUri'] = ''\n",
        "\n",
        "# get synthetic multi label sentences\n",
        "synth = pd.concat([pd.read_csv(work_dir('Data', s))\n",
        "                   for s in os.listdir(work_dir('Data')) if re.match(r'^multi\\_sentences\\_[0-9]+\\.csv$', s)])\n",
        "\n",
        "# gather all samples\n",
        "train_samples = pd.concat([\n",
        "    skills[['conceptUri', 'sentence']].assign(group=1),\n",
        "    nonskills[['conceptUri', 'sentence']].assign(group=1),\n",
        "    synth[['conceptUriPrimary', 'sentence']].rename(columns={'conceptUriPrimary': 'conceptUri'}).assign(group=2),\n",
        "    synth[['conceptUriSecondary', 'sentence']].rename(columns={'conceptUriSecondary': 'conceptUri'}).assign(group=3),\n",
        "    esco_en[['conceptUri', 'description']].rename(columns={'description': 'sentence'}).assign(group=4),\n",
        "    esco_da[['conceptUri', 'preferredLabel']].rename(columns={'preferredLabel': 'sentence'}).assign(group=5),\n",
        "    esco_da[['conceptUri', 'description']].rename(columns={'description': 'sentence'}).assign(group=6),\n",
        "], ignore_index=True).dropna().reset_index(drop=True)\n",
        "\n",
        "# assign id's to conceptUri's (-1 for empty, i.e. nonskill)\n",
        "uri_ids = [''] + proxies['conceptUri'].unique().tolist()\n",
        "uri_ids = dict(zip(uri_ids, range(-1, len(uri_ids)-1)))\n",
        "\n",
        "# map id's\n",
        "proxies['id'] = proxies['conceptUri'].map(uri_ids)\n",
        "train_samples['id'] = train_samples['conceptUri'].map(uri_ids)\n",
        "\n",
        "# sort by id for convenience\n",
        "train_samples.sort_values('id', inplace=True)\n",
        "\n",
        "# test split (with respect to multi labels represented as single labels with multi samples)\n",
        "test_samples = train_samples[train_samples['group'] == 1].groupby('id').sample(frac=0.1, random_state=7)\n",
        "test_samples = train_samples[train_samples['sentence'].isin(test_samples['sentence'])]\n",
        "train_samples = train_samples[~train_samples['sentence'].isin(test_samples['sentence'])].reset_index()\n",
        "\n",
        "# validation split\n",
        "val_samples = train_samples[train_samples['group'] == 1].groupby('id').sample(frac=0.1, random_state=7)\n",
        "val_samples = train_samples[train_samples['sentence'].isin(val_samples['sentence'])]\n",
        "train_samples = train_samples[~train_samples['sentence'].isin(val_samples['sentence'])]\n",
        "\n",
        "# check\n",
        "print('Sizes:', proxies.shape, train_samples.shape, val_samples.shape, test_samples.shape)\n",
        "print('Average labels per sentences:',\n",
        "      (train_samples['id'] > -1).sum() / train_samples['sentence'].nunique(),\n",
        "      (val_samples['id'] > -1).sum() / val_samples['sentence'].nunique(),\n",
        "      (test_samples['id'] > -1).sum() / test_samples['sentence'].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E62YCiZb8LhN",
        "outputId": "4121581d-9f4e-4a54-b9c0-e8012c52803f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes: (13896, 3) (398574, 5) (18779, 5) (25017, 4)\n",
            "Average labels per sentences: 1.4659312885361129 1.3400981052226604 1.5327504098680615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "7MiPSKFqTnjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(C.BASE_MODEL)"
      ],
      "metadata": {
        "id": "lot_XPiPTo0l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3w4fpmMc0Su"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define multi purpose dataset, e.g. proxies for predictor, samples for training, bench for evaluating\n",
        "class SkillData(Dataset):\n",
        "\n",
        "    # init that handles different usages, calculates length based on usage (num unique id for proxies)\n",
        "    def __init__(self, tokenizer, proxies=None, samples=None, seq_length=None, per_epoch=1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.proxies = proxies.reset_index(drop=True) if proxies is not None else None\n",
        "        self.samples = samples.reset_index(drop=True) if samples is not None else None\n",
        "        self.seq_length = seq_length\n",
        "        self.per_epoch = per_epoch\n",
        "        self.n = self.proxies['id'].nunique() if proxies is not None else len(self.samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n * self.per_epoch\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return i % self.n\n",
        "\n",
        "    # tokenize\n",
        "    def tokenize(self, sentences):\n",
        "        return tokenizer(sentences,\n",
        "                         padding='max_length' if self.seq_length else 'longest',\n",
        "                         truncation=True,\n",
        "                         max_length=self.seq_length,\n",
        "                         return_tensors='pt')\n",
        "\n",
        "    # collate proxy loading\n",
        "    def proxy_collate(self, ids):\n",
        "        proxies = self.proxies.loc[self.proxies['id'].isin(ids)].groupby('id').head(1)\n",
        "        tokens = self.tokenize(proxies['sentence'].tolist())\n",
        "        return {'id': torch.tensor(proxies['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask']}\n",
        "\n",
        "    # collate mnr training\n",
        "    def mnr_collate(self, ids):\n",
        "        proxies = self.proxies.loc[self.proxies['id'].isin(ids)].groupby('id').head(1)\n",
        "        samples = self.samples.loc[self.samples['id'].isin(ids)].groupby('id').sample(1)\n",
        "        nonskills = self.samples.loc[self.samples['id'] == -1].sample(len(samples))\n",
        "        samples = pd.concat([proxies, samples, nonskills], ignore_index=True)\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(samples['sentence'].unique(), range(len(samples['sentence'].unique()))))\n",
        "        sentence_ids = samples['sentence'].map(sentence_ids)\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence_ids.tolist(), dtype=torch.long)}\n",
        "\n",
        "    # collate direct training\n",
        "    def direct_collate(self, ids):\n",
        "        samples = self.samples.loc[self.samples['id'].isin(ids)].groupby('id').sample(1)\n",
        "        if (samples['id'] > -1).sum() > 1:\n",
        "            samples.loc[samples['id'] > -1] = self.augment(samples.loc[samples['id'] > -1])\n",
        "        if (samples['id'] == -1).sum() == 0:\n",
        "            samples.loc[samples['id'] == -1] = self.augment(samples.loc[samples['id'] == -1])\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(samples['sentence'].unique(), range(len(samples['sentence'].unique()))))\n",
        "        sentence_ids = samples['sentence'].map(sentence_ids)\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence_ids.tolist(), dtype=torch.long)}\n",
        "\n",
        "    # collate evaluation\n",
        "    def eval_collate(self, idx):\n",
        "        samples = self.samples.loc[self.samples.index.isin(idx)]\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(self.samples['sentence'].unique(), range(len(self.samples['sentence'].unique()))))\n",
        "        sentence_ids = samples['sentence'].map(sentence_ids)\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence_ids.tolist(), dtype=torch.long)}\n",
        "\n",
        "# initialize datasets\n",
        "\n",
        "proxy_data = SkillData(tokenizer=tokenizer,\n",
        "                       proxies=proxies,\n",
        "                       seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "train_data = SkillData(tokenizer=tokenizer,\n",
        "                       proxies=proxies,\n",
        "                       samples=train_samples,\n",
        "                       seq_length=C.SEQ_LENGTH,\n",
        "                       per_epoch=C.PER_EPOCH)\n",
        "\n",
        "val_data = SkillData(tokenizer=tokenizer,\n",
        "                     samples=val_samples,\n",
        "                     seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "test_data = SkillData(tokenizer=tokenizer,\n",
        "                      samples=test_samples,\n",
        "                      seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "# initialize dataloaders\n",
        "\n",
        "proxy_loader = DataLoader(proxy_data,\n",
        "                          batch_size=256,\n",
        "                          num_workers=C.NUM_WORKERS,\n",
        "                          prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                          collate_fn=proxy_data.proxy_collate,\n",
        "                          shuffle=False,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=C.BATCH_SIZE,\n",
        "                          num_workers=C.NUM_WORKERS,\n",
        "                          prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                          collate_fn=train_data.mnr_collate if C.TRAIN_METHOD == 'mnr' else train_data.direct_collate,\n",
        "                          shuffle=True,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "val_loader = DataLoader(val_data,\n",
        "                        batch_size=256,\n",
        "                        num_workers=C.NUM_WORKERS,\n",
        "                        prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                        collate_fn=val_data.eval_collate,\n",
        "                        shuffle=False,\n",
        "                        pin_memory=True,\n",
        "                        persistent_workers=True)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                         batch_size=256,\n",
        "                         num_workers=C.NUM_WORKERS,\n",
        "                         prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                         collate_fn=test_data.eval_collate,\n",
        "                         shuffle=False,\n",
        "                         pin_memory=True,\n",
        "                         persistent_workers=True)\n",
        "\n",
        "# check\n",
        "proxy_data.n, train_data.n, val_data.n, test_data.n"
      ],
      "metadata": {
        "id": "X8MjiIDeMr4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2130aaf-7801-416f-93bc-805b3c5778c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13896, 13896, 18779, 25017)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMOXbk_jnHXW"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "keVgpZLUFArQ"
      },
      "outputs": [],
      "source": [
        "# initialize base model\n",
        "base_model = AutoModel.from_pretrained(C.BASE_MODEL).to(C.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c69TNhUFBhC"
      },
      "source": [
        "# Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pNw_MhAV9nP_"
      },
      "outputs": [],
      "source": [
        "# define embedder\n",
        "class SkillEmbedder(nn.Module):\n",
        "\n",
        "    # initialize with base model and dropout rate\n",
        "    def __init__(self, base_model, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # embed using batch input_ids and attention_mask (including attention mean pooling!)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embeddings = self.base_model(input_ids, attention_mask).last_hidden_state#.mean(dim=1)\n",
        "        embeddings = (embeddings * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "        return self.dropout(embeddings)\n",
        "\n",
        "# init embedder\n",
        "embedder = SkillEmbedder(base_model=base_model, dropout_rate=C.DROPOUT_RATE).to(C.DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading embedder state\n",
        "state_dicts = torch.load(C.PATH('.pth'), weights_only=False)\n",
        "embedder.load_state_dict(state_dicts['embedder_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD5SqmohZe-2",
        "outputId": "68962e88-5e39-4b34-9709-c365d4fd99de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb8HZaFAQga4"
      },
      "source": [
        "# Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SeeKhq2gQht3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34435e20-6831-44ca-9c4d-dee44e51377d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Updating proxy embeddings: 100%|██████████| 55/55 [00:08<00:00,  6.79batch/s]\n"
          ]
        }
      ],
      "source": [
        "# define predictor\n",
        "class SkillPredictor(nn.Module):\n",
        "\n",
        "    # initialize embedder, proxy_loader and proxy embeddings\n",
        "    def __init__(self, embedder, proxy_loader, is_skill_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedder = embedder\n",
        "        self.proxy_loader = proxy_loader\n",
        "        self.is_skill_dim = is_skill_dim\n",
        "        self.embeddings = nn.Parameter(torch.zeros(self.proxy_loader.dataset.n,\n",
        "                                                  1,\n",
        "                                                  self.embedder.base_model.config.hidden_size,\n",
        "                                                  dtype=torch.half),\n",
        "                                       requires_grad=False)\n",
        "\n",
        "    # update proxy embeddings\n",
        "    def update_embeddings(self):\n",
        "\n",
        "        training = embedder.training\n",
        "        embedder.eval()\n",
        "        pbar = tqdm(proxy_loader, desc=f'Updating proxy embeddings', unit='batch')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in pbar:\n",
        "                with torch.amp.autocast(str(self.embeddings.device)):\n",
        "                    batch = {k: v.to(self.embeddings.device) for k, v in batch.items()}\n",
        "                    embeddings = embedder(batch['input_ids'], batch['attention_mask']).to(torch.half)\n",
        "                    for id in batch['id'].unique():\n",
        "                        skill_embeddings = embeddings[batch['id'] == id]\n",
        "                        self.embeddings[id] = skill_embeddings\n",
        "\n",
        "        embedder.train(training)\n",
        "\n",
        "    # predict is_skill from n'th dimension(s) and skill_id from proxy embedding similarity\n",
        "    def forward(self, embeddings, include='both', logits=False):\n",
        "\n",
        "        if include in ('both', 'all', 'is_skill'):\n",
        "            is_skill = embeddings[:, -self.is_skill_dim:].mean(dim=-1)\n",
        "            is_skill = is_skill if logits else F.sigmoid(is_skill)\n",
        "            if include == 'is_skill':\n",
        "                return is_skill\n",
        "\n",
        "        if include in ('both', 'all', 'skill_id'):\n",
        "            sims = F.cosine_similarity(embeddings.unsqueeze(1).unsqueeze(1),\n",
        "                                       self.embeddings,\n",
        "                                       dim=-1).max(dim=-1)[0]\n",
        "            skill_id = sims if logits else F.softmax(sims, dim=-1)\n",
        "            if include == 'skill_id':\n",
        "                return skill_id\n",
        "\n",
        "        return is_skill, skill_id\n",
        "\n",
        "# init predictor\n",
        "predictor = SkillPredictor(embedder=embedder, proxy_loader=proxy_loader, is_skill_dim=C.IS_SKILL_DIM).to(C.DEVICE)\n",
        "predictor.update_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criterion"
      ],
      "metadata": {
        "id": "K2utPywi9C2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define criterion\n",
        "class SkillCriterion(nn.Module):\n",
        "    def __init__(self, is_skill_fp_penalty=0.0, skill_id_temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.is_skill_fp_penalty = is_skill_fp_penalty\n",
        "        self.skill_id_temperature = skill_id_temperature\n",
        "\n",
        "    # calculate loss for is_skill (with false positives penalty) and skill_id (with temperature)\n",
        "    def forward(self, is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels):\n",
        "        is_skill_loss = F.binary_cross_entropy_with_logits(is_skill_logits.float(),\n",
        "                                                           is_skill_labels.float(),\n",
        "                                                           reduction='none')\n",
        "        is_skill_loss *= 1 + ((is_skill_logits > 0.0) & (~is_skill_labels.bool())).float() * self.is_skill_fp_penalty\n",
        "        is_skill_loss = is_skill_loss.mean()\n",
        "        if (skill_id_labels > -1).sum() > 0:\n",
        "            skill_id_loss = F.cross_entropy(skill_id_logits / self.skill_id_temperature, skill_id_labels, ignore_index=-1)\n",
        "        else:\n",
        "            skill_id_loss = torch.tensor(0.0, device=is_skill_logits.device)\n",
        "        return C.IS_SKILL_WEIGHT * is_skill_loss + skill_id_loss, {'is_skill_loss': is_skill_loss.item(), 'skill_id_loss': skill_id_loss.item()}\n",
        "\n",
        "# init criterion\n",
        "criterion = SkillCriterion(is_skill_fp_penalty=C.IS_SKILL_FP_PENALTY, skill_id_temperature=C.SKILL_ID_TEMP_INITIAL).to(C.DEVICE)"
      ],
      "metadata": {
        "id": "kLvh55L89Ey8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "FxdwSjrZ5sr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define metrics\n",
        "class SkillMetrics(nn.Module):\n",
        "    def __init__(self, atp_temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.atp_temperature = atp_temperature\n",
        "\n",
        "    # calculate metrics\n",
        "    def forward(self, is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels, sentences):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # calculate is_skill precision and recall\n",
        "            tp = ((is_skill_logits > 0.0) & is_skill_labels).sum().item()\n",
        "            fp = ((is_skill_logits > 0.0) & ~is_skill_labels).sum().item()\n",
        "            fn = ((is_skill_logits <= 0.0) & is_skill_labels).sum().item()\n",
        "            is_skill_precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            is_skill_recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "            # group entries by unique sentences\n",
        "            sentence_to_indices = {}\n",
        "            sentence_list = sentences.tolist()\n",
        "            for i, sid in enumerate(sentence_list):\n",
        "                sentence_to_indices.setdefault(sid, []).append(i)\n",
        "\n",
        "            # initialize skill_id metrics\n",
        "            mrr_sum, rp5_sum, atp_sum, count = 0.0, 0.0, 0.0, 0\n",
        "\n",
        "            # softmax probabilities and ranking\n",
        "            probs = F.softmax(skill_id_logits / self.atp_temperature, dim=1)\n",
        "            ranks = torch.argsort(skill_id_logits, dim=1, descending=True)\n",
        "\n",
        "            # process metrics per unique sentence\n",
        "            for sid, indices in sentence_to_indices.items():\n",
        "\n",
        "                # collect skill labels for this sentence\n",
        "                sentence_labels = {skill_id_labels[i].item() for i in indices if skill_id_labels[i] > -1}\n",
        "\n",
        "                # check for any labels or skip\n",
        "                if len(sentence_labels) == 0:\n",
        "                    continue\n",
        "\n",
        "                # ATP calculation (Average True Probability)\n",
        "                sentence_atp = probs[indices[0], list(sentence_labels)].sum().item()\n",
        "                atp_sum += sentence_atp / len(sentence_labels)\n",
        "\n",
        "                # MRR calculation (use only max ranking):\n",
        "                sentence_mrr = 0.0\n",
        "                for pos, pred in enumerate(ranks[indices[0]].tolist(), start=1):\n",
        "                    if pred in sentence_labels:\n",
        "                        sentence_mrr = 1.0 / pos\n",
        "                        break\n",
        "                mrr_sum += sentence_mrr\n",
        "\n",
        "                # RP@5 calculation\n",
        "                top_5_preds = set(ranks[indices[0], :5].tolist())\n",
        "                top_k_correct = len(sentence_labels & top_5_preds)\n",
        "                rp5_sum += top_k_correct / min(5, len(sentence_labels))\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            # finalize metrics\n",
        "            if count > 0:\n",
        "                skill_id_atp = atp_sum / count\n",
        "                skill_id_rp5 = rp5_sum / count\n",
        "                skill_id_mrr = mrr_sum / count\n",
        "            else:\n",
        "                skill_id_atp, skill_id_rp5, skill_id_mrr = None, None, None\n",
        "\n",
        "        return {\n",
        "            'is_skill_pre': is_skill_precision,\n",
        "            'is_skill_rec': is_skill_recall,\n",
        "            'skill_id_atp': skill_id_atp,\n",
        "            'skill_id_rp5': skill_id_rp5,\n",
        "            'skill_id_mrr': skill_id_mrr\n",
        "        }\n",
        "\n",
        "# init metrics\n",
        "metrics = SkillMetrics(atp_temperature=C.ATP_TEMPERATURE).to(C.DEVICE)"
      ],
      "metadata": {
        "id": "fU-OnTX73HlC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ydnFzovbmG-"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k9K9SpX6awC9"
      },
      "outputs": [],
      "source": [
        "# set n last layers trainable\n",
        "for idx, layer in enumerate(base_model.encoder.layer):\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = idx >= len(base_model.encoder.layer) - C.N_LAYERS\n",
        "\n",
        "# calculate layers to optimize learning for\n",
        "n_layers = C.N_LAYERS - len(base_model.encoder.layer)\n",
        "\n",
        "# create param groups for optimizer with layer-wise learning rate  (factor applied per layer)\n",
        "param_groups = reversed([\n",
        "    {'params': base_model.encoder.layer[i].parameters(), 'lr': C.LR_INITIAL * C.LR_LAYER_FACTOR**-(i + 1)} for i in range(n_layers, 0)\n",
        "])\n",
        "\n",
        "# adam with weight decay, default settings\n",
        "optimizer = torch.optim.AdamW(param_groups, weight_decay=C.WEIGHT_DECAY_RATE)\n",
        "\n",
        "# mixed precision scaler\n",
        "scaler = torch.amp.GradScaler(str(C.DEVICE))\n",
        "\n",
        "# collection of mixed precision backward pass calls\n",
        "def backward(loss, optimizer, scaler):\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    clip_grad_norm_(embedder.parameters(), max_norm=1.0)\n",
        "    scaler.unscale_(optimizer)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dDtnik0-opH"
      },
      "source": [
        "# Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EfL7605b-nu6"
      },
      "outputs": [],
      "source": [
        "# define module for logging\n",
        "class Logger(nn.Module):\n",
        "\n",
        "    def __init__(self, optimizer, path):\n",
        "        super().__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.path = path\n",
        "\n",
        "    def forward(self, epoch, data):\n",
        "\n",
        "        # init log data fields\n",
        "        log_data = {'datetime': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'epoch': epoch}\n",
        "\n",
        "        # gather log data\n",
        "        log_data |= data\n",
        "\n",
        "        # init log with header\n",
        "        if not os.path.exists(self.path):\n",
        "            with open(self.path, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(log_data.keys())\n",
        "\n",
        "        # log data for epoch\n",
        "        with open(self.path, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(log_data.values())\n",
        "            f.flush()\n",
        "\n",
        "# initialize logging\n",
        "logger = Logger(optimizer, path=C.PATH('.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "y_5iOXU-zz4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decision variables\n",
        "max_metrics = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "# run through epochs\n",
        "for epoch in range(C.EPOCHS):\n",
        "\n",
        "    # training mode\n",
        "    embedder.train()\n",
        "\n",
        "    # init stats and progress bar\n",
        "    train_stats = {}\n",
        "    model_stats = {}\n",
        "    pbar = tqdm(train_loader, desc=f'Training (epoch {epoch+1}/{C.EPOCHS})', unit='batch')\n",
        "\n",
        "    # train\n",
        "    for num_batch, batch in enumerate(pbar):\n",
        "        with torch.amp.autocast(str(C.DEVICE)):\n",
        "\n",
        "            # send batch to GPU\n",
        "            batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            # generate embeddings\n",
        "            embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "            if C.TRAIN_METHOD == 'mnr':\n",
        "\n",
        "                # separate\n",
        "                proxies, samples, nonskills = torch.chunk(embeddings, 3)\n",
        "\n",
        "                # predictions\n",
        "                is_skill_logits = predictor(torch.cat([samples, nonskills], dim=0), include='is_skill', logits=True)\n",
        "                skill_id_logits = F.cosine_similarity(samples.unsqueeze(1),\n",
        "                                                      proxies.unsqueeze(0),\n",
        "                                                      dim=-1)\n",
        "\n",
        "                # truth\n",
        "                is_skill_labels = (batch['id'] > -1)[len(proxies):]\n",
        "                skill_id_labels = torch.arange(len(samples), device=C.DEVICE)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # predictions\n",
        "                is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "                # truth\n",
        "                is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "            # run batch\n",
        "            loss, stats = criterion(is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels)\n",
        "\n",
        "        # backward pass\n",
        "        backward(loss, optimizer, scaler)\n",
        "\n",
        "        # update progress\n",
        "        train_stats = {\n",
        "            'train_loss': [loss.item()] + train_stats.setdefault('loss', []),\n",
        "            'train_is_skill_loss': [stats['is_skill_loss']] + train_stats.setdefault('is_skill_loss', []),\n",
        "            'train_skill_id_loss': [stats['skill_id_loss']] + train_stats.setdefault('skill_id_loss', []),\n",
        "            'lr': [optimizer.param_groups[0]['lr']] + model_stats.setdefault('lr', []),\n",
        "            'patience': [C.PATIENCE - patience_counter],\n",
        "            'temperature': [criterion.skill_id_temperature] + model_stats.setdefault('temperature', []),\n",
        "        }\n",
        "\n",
        "        pbar.set_postfix({k: sum(v) / len(v) for k, v in train_stats.items()})\n",
        "\n",
        "        # decay temperature\n",
        "        criterion.skill_id_temperature = max(criterion.skill_id_temperature * C.SKILL_ID_TEMP_FACTOR, C.SKILL_ID_TEMP)\n",
        "\n",
        "        # warmup lr\n",
        "        for param_group in optimizer.param_groups:\n",
        "            if param_group['lr'] >= C.LR or patience_counter > 0:\n",
        "                break\n",
        "            param_group['lr'] = min(param_group['lr'] * C.LR_WARMUP_FACTOR, C.LR)\n",
        "\n",
        "    # clean up\n",
        "    del batch, embeddings, is_skill_logits, skill_id_logits, is_skill_labels, skill_id_labels, loss, stats\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # update proxy embeddings after training / before validation\n",
        "    predictor.update_embeddings()\n",
        "\n",
        "    # validation mode\n",
        "    embedder.eval()\n",
        "\n",
        "    # init stats and progress bar\n",
        "    val_stats = {}\n",
        "    pbar = tqdm(val_loader, desc=f'Validation (epoch {epoch+1}/{C.EPOCHS})', unit='batch')\n",
        "\n",
        "    # results warehouse\n",
        "    logits_and_labels = dict()\n",
        "\n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "        for num_batch, batch in enumerate(pbar):\n",
        "\n",
        "            # send batch to GPU\n",
        "            batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            # generate embeddings\n",
        "            embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "            # predictions\n",
        "            is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "            # truth\n",
        "            is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "            # save logits and labels\n",
        "            logits_and_labels.setdefault('is_skill_logits', []).append(is_skill_logits)\n",
        "            logits_and_labels.setdefault('is_skill_labels', []).append(is_skill_labels)\n",
        "            logits_and_labels.setdefault('skill_id_logits', []).append(skill_id_logits)\n",
        "            logits_and_labels.setdefault('skill_id_labels', []).append(skill_id_labels)\n",
        "            logits_and_labels.setdefault('sentence', []).append(batch['sentence'])\n",
        "\n",
        "        # run batch\n",
        "        loss, stats = criterion(*[torch.cat(v) for v in list(logits_and_labels.values())[:4]])\n",
        "        val_metrics = metrics(*[torch.cat(v) for v in logits_and_labels.values()])\n",
        "        stats = {'loss': loss.item()} | stats | val_metrics\n",
        "\n",
        "        # update progress\n",
        "        val_stats = {k: [v] + val_stats.setdefault(k, []) for k, v in stats.items() if v is not None}\n",
        "        print('Validation:', ', '.join([f'{k} = {str(sum(v) / len(v))}' for k, v in val_stats.items()]))\n",
        "\n",
        "    # clean up\n",
        "    del batch, embeddings, is_skill_logits, skill_id_logits, is_skill_labels, skill_id_labels, logits_and_labels, loss, stats\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # finalize stats\n",
        "    train_stats = {k: sum(v) / len(v) for k, v in train_stats.items()}\n",
        "    val_stats = {k: sum(v) / len(v) for k, v in val_stats.items()}\n",
        "\n",
        "    # logging\n",
        "    logger(epoch=epoch + 1, data=val_stats | train_stats)\n",
        "\n",
        "    # save or break (if break, load best weights and restore embeddings)\n",
        "    avg_metrics = sum([v for k, v in val_stats.items() if k in val_metrics.keys()])\n",
        "    if avg_metrics > max_metrics:\n",
        "        patience_counter = 0\n",
        "        max_metrics = avg_metrics\n",
        "        torch.save({'embedder_state_dict': embedder.state_dict(),\n",
        "                    'predictor_state_dict': predictor.state_dict()}, C.PATH('.pth'))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= C.PATIENCE:\n",
        "            state_dicts = torch.load(C.PATH('.pth'), weights_only=False)\n",
        "            embedder.load_state_dict(state_dicts['embedder_state_dict'])\n",
        "            predictor.load_state_dict(state_dicts['predictor_state_dict'])\n",
        "            break\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] *= C.LR_REDUCE_FACTOR"
      ],
      "metadata": {
        "id": "r_cpM1nHz1G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a62ce82-d199-44d5-e8a8-5ce42d303607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 1/100):  69%|██████▉   | 1005/1448 [05:45<02:32,  2.91batch/s, train_loss=4.17, train_is_skill_loss=0.692, train_skill_id_loss=4, lr=1.65e-8, patience=3, temperature=0.904]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Q6_uQtvN7a4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init stats and progress bar\n",
        "test_stats = {}\n",
        "pbar = tqdm(test_loader, desc=f'Test', unit='batch')\n",
        "\n",
        "# results warehouse\n",
        "logits_and_labels = dict()\n",
        "\n",
        "# test\n",
        "with torch.no_grad():\n",
        "    for num_batch, batch in enumerate(pbar):\n",
        "\n",
        "        # test mode\n",
        "        embedder.eval()\n",
        "\n",
        "        # send batch to GPU\n",
        "        batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        # generate embeddings\n",
        "        embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        # predictions\n",
        "        is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "        # truth\n",
        "        is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "        # save logits and labels\n",
        "        logits_and_labels.setdefault('is_skill_logits', []).append(is_skill_logits)\n",
        "        logits_and_labels.setdefault('is_skill_labels', []).append(is_skill_labels)\n",
        "        logits_and_labels.setdefault('skill_id_logits', []).append(skill_id_logits)\n",
        "        logits_and_labels.setdefault('skill_id_labels', []).append(skill_id_labels)\n",
        "        logits_and_labels.setdefault('sentence', []).append(batch['sentence'])\n",
        "\n",
        "    # concatenate\n",
        "    logits_and_labels = {k: torch.cat(v) for k, v in logits_and_labels.items()}\n",
        "\n",
        "    # run batch\n",
        "    loss, stats = criterion(*list(logits_and_labels.values())[:4])\n",
        "    stats = {'loss': loss.item()} | stats | metrics(*list(logits_and_labels.values())[:5])\n",
        "\n",
        "    # update progress\n",
        "    test_stats = {k: [v] + test_stats.setdefault(k, []) for k, v in stats.items() if v is not None}\n",
        "    print('Test:', ', '.join([f'{k} = {str(sum(v) / len(v))}' for k, v in test_stats.items()]))\n",
        "\n",
        "# finalize stats\n",
        "test_stats = {k: sum(v) / len(v) for k, v in test_stats.items()}\n",
        "\n",
        "# logging\n",
        "logger(epoch='test', data=test_stats)"
      ],
      "metadata": {
        "id": "HG1_fRBXT0SK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPS0tLmv9rpz37416EQB41j"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}