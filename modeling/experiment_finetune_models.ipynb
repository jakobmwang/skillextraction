{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8VR8TxNxqk7"
      },
      "source": [
        "# Installation and Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJnIx8Q8xY6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130099c2-1f13-4d79-a673-fccf2fbd19f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# dependencies\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# file management\n",
        "drive.mount('/content/drive')\n",
        "WORK_DIR = '/content/drive/MyDrive/Projects/skillextraction'\n",
        "\n",
        "# work dir shortcut function\n",
        "def work_dir(*args):\n",
        "    return os.path.join(WORK_DIR, *args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTkVCN_oudy"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config container\n",
        "class C:\n",
        "\n",
        "    # architecture\n",
        "    BASE_MODEL = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "    PROXY_GROUPS = [1,2,3,4,5,6,7,8,9,10,11] # label_en = [1], label_da = [2], desc_en = [3], desc_da = [4]\n",
        "    SAMPLE_GROUPS = [1,2,3,4,5,6,7,8,9,10,11] # article_en = [5], article_da = [6], extra_da = [7], alts_en = [8], alts_da = [9], multi_1 = [10], multi_2 = [11]\n",
        "    AVERAGE_EMBEDDINGS = True\n",
        "    SEQ_LENGTH = 256 # 99.9th percentile of bench, 100% of others\n",
        "    IS_SKILL_DIM = 8 # how many dimensions are read out for is_skill\n",
        "    ATP_TEMPERATURE = 0.005 # average true probability metric temperature\n",
        "\n",
        "    # training\n",
        "    N_LAYERS = 5\n",
        "    LR = 1e-6\n",
        "    LR_INITIAL = 1e-8\n",
        "    LR_LAYER_FACTOR = 0.5\n",
        "    LR_REDUCE_FACTOR = 0.1\n",
        "    LR_WARMUP_FACTOR = 1.0005\n",
        "    TRAIN_METHOD = 'mnr' # 'direct', 'mnr'\n",
        "    EPOCHS = 100\n",
        "    PER_EPOCH = 50 # training samples per epoch per skill\n",
        "    BATCH_SIZE = 128\n",
        "    PATIENCE = 3 # early stopping\n",
        "    IS_SKILL_WEIGHT = 0.25 # loss coefficient\n",
        "    IS_SKILL_FP_PENALTY = 0.0005 # false positives loss multipler\n",
        "    SKILL_ID_TEMP = 0.05 # loss temperature\n",
        "    SKILL_ID_TEMP_INITIAL = 1.0\n",
        "    SKILL_ID_TEMP_FACTOR = 0.9999\n",
        "\n",
        "    # regularization\n",
        "    AUGMENT_RATE = 1.0\n",
        "    DROPOUT_RATE = 0.1\n",
        "    WEIGHT_DECAY_RATE = 0.1\n",
        "\n",
        "    # system\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    NUM_WORKERS = 2\n",
        "    PREFETCH_FACTOR = 1\n",
        "\n",
        "    # export path\n",
        "    def PATH(postfix=''):\n",
        "        return work_dir('experiments', '-'.join(str(v).replace('/', '-') for k, v in vars(C).items() if k.isupper() and k != 'PATH') + postfix)\n",
        "\n",
        "# check config-aggregated path\n",
        "C.PATH('.csv')"
      ],
      "metadata": {
        "id": "imF1lwZErTY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "693aadd9-455e-41cf-8e30-8866f34172dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Projects/skillextraction/experiments/sentence-transformers-paraphrase-multilingual-mpnet-base-v2-[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]-[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]-True-256-8-0.005-5-1e-06-1e-08-0.5-0.1-1.0005-mnr-100-50-128-3-0.25-0.0005-0.05-1.0-0.9999-1.0-0.1-0.1-cuda-2-1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r531y35IvrYe"
      },
      "source": [
        "# Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-organized data\n",
        "skills = pd.read_json(work_dir('Data', 'skills.json'), orient='records', lines=True)\n",
        "nonskills = pd.read_json(work_dir('Data', 'nonskills.json'), orient='records', lines=True)\n",
        "bench = pd.read_json(work_dir('Data', 'bench.json'), orient='records', lines=True)\n",
        "\n",
        "# assign id's to conceptUri's\n",
        "uri_ids = skills['conceptUri'].unique()\n",
        "uri_ids = dict(zip(uri_ids, range(len(uri_ids))))\n",
        "\n",
        "# map id's for skills and bench\n",
        "skills['id'] = skills['conceptUri'].map(uri_ids)\n",
        "bench['id'] = bench['conceptUri'].map(uri_ids)\n",
        "\n",
        "# sort for convenience\n",
        "skills = skills.sort_values('id').reset_index(drop=True)\n",
        "bench = bench.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "# padding id for nonskills\n",
        "nonskills['id'] = -1\n",
        "\n",
        "# extract skills from bench for validation and test\n",
        "validation = bench[bench['group'].isin([1, 3, 6, 8])].reset_index(drop=True)\n",
        "test = bench[bench['group'].isin([2, 4, 5, 7, 9, 10])].reset_index(drop=True)\n",
        "\n",
        "# extract and append nonskills for validation from manually annotated\n",
        "val_nonskills = nonskills[nonskills['group'].isin([2, 3])].sample(len(validation), random_state=7)\n",
        "nonskills.drop(val_nonskills.index, inplace=True)\n",
        "validation = pd.concat([validation, val_nonskills], ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "# extract and append nonskills for test from manually annotated\n",
        "test_nonskills = nonskills[nonskills['group'].isin([2, 3])].sample(len(test), random_state=7)\n",
        "nonskills.drop(test_nonskills.index, inplace=True)\n",
        "test = pd.concat([test, test_nonskills], ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "# check\n",
        "print(skills.shape, nonskills.shape, validation.shape, test.shape)\n",
        "skills.columns, nonskills.columns, validation.columns, test.columns"
      ],
      "metadata": {
        "id": "gzG5DbdL15Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbdd9ca-fe58-4ab8-d80a-300f9f7e355b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(990700, 4) (95818, 3) (1110, 4) (7254, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['conceptUri', 'sentence', 'group', 'id'], dtype='object'),\n",
              " Index(['group', 'sentence', 'id'], dtype='object'),\n",
              " Index(['conceptUri', 'group', 'sentence', 'id'], dtype='object'),\n",
              " Index(['conceptUri', 'group', 'sentence', 'id'], dtype='object'))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "7MiPSKFqTnjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(C.BASE_MODEL)"
      ],
      "metadata": {
        "id": "lot_XPiPTo0l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3w4fpmMc0Su"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define multi purpose dataset, e.g. proxies for predictor, samples for training, bench for evaluating\n",
        "class SkillData(Dataset):\n",
        "\n",
        "    # init that handles different usages, calculates length based on usage (num unique id for proxies)\n",
        "    def __init__(self, tokenizer, proxies=None, samples=None, nonskills=None, augment_rate=1.0, seq_length=None, per_epoch=1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.proxies = proxies.reset_index(drop=True) if proxies is not None else None\n",
        "        self.samples = samples.reset_index(drop=True) if samples is not None else None\n",
        "        self.nonskills = nonskills.reset_index(drop=True) if nonskills is not None else None\n",
        "        self.augment_rate = augment_rate\n",
        "        self.seq_length = seq_length\n",
        "        self.per_epoch = per_epoch\n",
        "        # calculate number of proxies per skill and number of samples (unique for train, all for eval)\n",
        "        self.n_proxies = self.proxies['id'].value_counts().min() if proxies is not None else 0\n",
        "        self.n = self.proxies['id'].nunique() if proxies is not None else len(self.samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n * self.per_epoch\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return i % self.n\n",
        "\n",
        "    # shortcut tokenize\n",
        "    def tokenize(self, sentences):\n",
        "        return tokenizer(sentences,\n",
        "                         padding='max_length' if self.seq_length else 'longest',\n",
        "                         truncation=True,\n",
        "                         max_length=self.seq_length,\n",
        "                         return_tensors='pt')\n",
        "\n",
        "    # augment into pairs of sentences, even / odd\n",
        "    def augment(self, samples):\n",
        "        if random.random() >= self.augment_rate:\n",
        "            return self.samples\n",
        "        s1 = samples['sentence'].iloc[::2].reset_index(drop=True)\n",
        "        s2 = samples['sentence'].iloc[1::2].reset_index(drop=True)\n",
        "        s3 = pd.Series()\n",
        "        if len(s1) > len(s2):\n",
        "            s3 = s1.iloc[-1:]\n",
        "            s1 = s1.iloc[:-1]\n",
        "        return samples.reset_index(drop=True).assign(\n",
        "            sentence=pd.concat([np.repeat(s1 + ' ' + s2, 2), s3], ignore_index=True).reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "    # collate proxy loading\n",
        "    def proxy_collate(self, ids):\n",
        "        proxies = self.proxies.loc[self.proxies['id'].isin(ids)].groupby('id').sample(self.n_proxies)\n",
        "        tokens = self.tokenize(proxies['sentence'].tolist())\n",
        "        return {'id': torch.tensor(proxies['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask']}\n",
        "\n",
        "    # collate mnr training\n",
        "    def mnr_collate(self, ids):\n",
        "        samples = self.samples.loc[self.samples['id'].isin(ids)].groupby('id').sample(1)\n",
        "        samples = self.augment(samples)\n",
        "        samples = pd.concat([samples, self.proxies.loc[self.proxies['id'].isin(ids)].groupby('id').sample(1)], ignore_index=True) # only one proxy for mnr\n",
        "        samples = pd.concat([samples, self.nonskills.sample(len(samples), random_state=7)], ignore_index=True)\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(samples['sentence'].unique(), range(len(samples['sentence'].unique()))))\n",
        "        sentence = samples['sentence'].map(sentence_ids)\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence.tolist(), dtype=torch.long)}\n",
        "\n",
        "    # collate direct training\n",
        "    def direct_collate(self, ids):\n",
        "        samples = self.samples.loc[self.samples['id'].isin(ids)].groupby('id').sample(1)\n",
        "        samples = self.augment(samples)\n",
        "        samples = pd.concat([samples, self.nonskills.sample(len(samples), random_state=7)], ignore_index=True)\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(samples['sentence'].unique(), range(len(samples['sentence'].unique()))))\n",
        "        sentence = samples['sentence'].map(sentence_ids)\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence.tolist(), dtype=torch.long)}\n",
        "\n",
        "    # collate evaluation\n",
        "    def eval_collate(self, idx):\n",
        "        samples = self.samples.loc[self.samples.index.isin(idx)]\n",
        "        tokens = self.tokenize(samples['sentence'].tolist())\n",
        "        sentence_ids = dict(zip(self.samples['sentence'].unique(), range(len(self.samples['sentence'].unique()))))\n",
        "        sentence = samples['sentence'].map(sentence_ids)\n",
        "        group = ((samples['id'] > -1) * samples['group']).tolist()\n",
        "        return {'id': torch.tensor(samples['id'].tolist(), dtype=torch.long),\n",
        "                'input_ids': tokens['input_ids'],\n",
        "                'attention_mask': tokens['attention_mask'],\n",
        "                'sentence': torch.tensor(sentence.tolist(), dtype=torch.long),\n",
        "                'group': torch.tensor(group, dtype=torch.long)}\n",
        "\n",
        "# initialize datasets\n",
        "\n",
        "proxy_data = SkillData(tokenizer=tokenizer,\n",
        "                       proxies=skills.loc[skills['group'].isin(C.PROXY_GROUPS)],\n",
        "                       seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "train_data = SkillData(tokenizer=tokenizer,\n",
        "                       proxies=proxy_data.proxies,\n",
        "                       samples=skills.loc[skills['group'].isin(C.SAMPLE_GROUPS)],\n",
        "                       nonskills=nonskills,\n",
        "                       per_epoch=C.PER_EPOCH,\n",
        "                       augment_rate=C.AUGMENT_RATE)\n",
        "\n",
        "val_data = SkillData(tokenizer=tokenizer,\n",
        "                     samples=validation,\n",
        "                     seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "test_data = SkillData(tokenizer=tokenizer,\n",
        "                      samples=test,\n",
        "                      seq_length=C.SEQ_LENGTH)\n",
        "\n",
        "# initialize dataloaders\n",
        "\n",
        "proxy_loader = DataLoader(proxy_data,\n",
        "                          batch_size=64, # C.BATCH_SIZE,\n",
        "                          num_workers=C.NUM_WORKERS,\n",
        "                          prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                          collate_fn=proxy_data.proxy_collate,\n",
        "                          shuffle=False,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=C.BATCH_SIZE // 2 if C.TRAIN_METHOD == 'mnr' else C.BATCH_SIZE,\n",
        "                          num_workers=C.NUM_WORKERS,\n",
        "                          prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                          collate_fn=train_data.mnr_collate if C.TRAIN_METHOD == 'mnr' else train_data.direct_collate,\n",
        "                          shuffle=True,\n",
        "                          pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "val_loader = DataLoader(val_data,\n",
        "                        batch_size=C.BATCH_SIZE,\n",
        "                        num_workers=C.NUM_WORKERS,\n",
        "                        prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                        collate_fn=val_data.eval_collate,\n",
        "                        shuffle=False,\n",
        "                        pin_memory=True,\n",
        "                        persistent_workers=True)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                         batch_size=C.BATCH_SIZE,\n",
        "                         num_workers=C.NUM_WORKERS,\n",
        "                         prefetch_factor=C.PREFETCH_FACTOR,\n",
        "                         collate_fn=test_data.eval_collate,\n",
        "                         shuffle=False,\n",
        "                         pin_memory=True,\n",
        "                         persistent_workers=True)\n",
        "\n",
        "# check\n",
        "proxy_data.n, train_data.n, val_data.n, test_data.n, proxy_data.n_proxies"
      ],
      "metadata": {
        "id": "X8MjiIDeMr4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6057f6eb-bc58-4dd1-f9fa-1ab92101f78e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13813, 13813, 1110, 7254, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMOXbk_jnHXW"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "keVgpZLUFArQ"
      },
      "outputs": [],
      "source": [
        "# initialize base model\n",
        "base_model = AutoModel.from_pretrained(C.BASE_MODEL).to(C.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c69TNhUFBhC"
      },
      "source": [
        "# Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pNw_MhAV9nP_"
      },
      "outputs": [],
      "source": [
        "# define embedder\n",
        "class SkillEmbedder(nn.Module):\n",
        "\n",
        "    # initialize with base model and dropout rate\n",
        "    def __init__(self, base_model, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    # embed using batch input_ids and attention_mask (including attention mean pooling!)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embeddings = self.base_model(input_ids, attention_mask).last_hidden_state#.mean(dim=1)\n",
        "        embeddings = (embeddings * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
        "        return self.dropout(embeddings)\n",
        "\n",
        "# init embedder\n",
        "embedder = SkillEmbedder(base_model=base_model, dropout_rate=C.DROPOUT_RATE).to(C.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb8HZaFAQga4"
      },
      "source": [
        "# Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SeeKhq2gQht3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9a55ce-9917-400f-a614-9b94c9b8732d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:28<00:00,  1.03batch/s]\n"
          ]
        }
      ],
      "source": [
        "# define predictor\n",
        "class SkillPredictor(nn.Module):\n",
        "\n",
        "    # initialize embedder, proxy_loader and proxy embeddings\n",
        "    def __init__(self, embedder, proxy_loader, average, is_skill_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedder = embedder\n",
        "        self.proxy_loader = proxy_loader\n",
        "        self.average = average\n",
        "        self.is_skill_dim = is_skill_dim\n",
        "        self.embeddings = nn.Parameter(torch.zeros(self.proxy_loader.dataset.n,\n",
        "                                                  1 if self.average else self.proxy_loader.dataset.n_proxies,\n",
        "                                                  self.embedder.base_model.config.hidden_size,\n",
        "                                                  dtype=torch.half),\n",
        "                                       requires_grad=False)\n",
        "\n",
        "    # update proxy embeddings\n",
        "    def update_embeddings(self):\n",
        "\n",
        "        training = embedder.training\n",
        "        embedder.eval()\n",
        "        pbar = tqdm(proxy_loader, desc=f'Updating proxy embeddings', unit='batch')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in pbar:\n",
        "                with torch.amp.autocast(str(self.embeddings.device)):\n",
        "                    batch = {k: v.to(self.embeddings.device) for k, v in batch.items()}\n",
        "                    embeddings = embedder(batch['input_ids'], batch['attention_mask']).to(torch.half)\n",
        "                    for id in batch['id'].unique():\n",
        "                        skill_embeddings = embeddings[batch['id'] == id]\n",
        "                        self.embeddings[id] = skill_embeddings.mean(dim=0, keepdim=True) if self.average else skill_embeddings\n",
        "\n",
        "        embedder.train(training)\n",
        "\n",
        "    # predict is_skill from n'th dimension(s) and skill_id from proxy embedding similarity\n",
        "    def forward(self, embeddings, include='both', logits=False):\n",
        "\n",
        "        if include in ('both', 'all', 'is_skill'):\n",
        "            is_skill = embeddings[:, -self.is_skill_dim:].mean(dim=-1)\n",
        "            is_skill = is_skill if logits else F.sigmoid(is_skill)\n",
        "            if include == 'is_skill':\n",
        "                return is_skill\n",
        "\n",
        "        if include in ('both', 'all', 'skill_id'):\n",
        "            sims = F.cosine_similarity(embeddings.unsqueeze(1).unsqueeze(1),\n",
        "                                       self.embeddings,\n",
        "                                       dim=-1).max(dim=-1)[0]\n",
        "            skill_id = sims if logits else F.softmax(sims, dim=-1)\n",
        "            if include == 'skill_id':\n",
        "                return skill_id\n",
        "\n",
        "        return is_skill, skill_id\n",
        "\n",
        "# init predictor\n",
        "predictor = SkillPredictor(embedder=embedder, proxy_loader=proxy_loader, average=C.AVERAGE_EMBEDDINGS, is_skill_dim=C.IS_SKILL_DIM).to(C.DEVICE)\n",
        "predictor.update_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criterion"
      ],
      "metadata": {
        "id": "K2utPywi9C2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define criterion\n",
        "class SkillCriterion(nn.Module):\n",
        "    def __init__(self, is_skill_fp_penalty=0.0, skill_id_temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.is_skill_fp_penalty = is_skill_fp_penalty\n",
        "        self.skill_id_temperature = skill_id_temperature\n",
        "\n",
        "    # calculate loss for is_skill (with false positives penalty) and skill_id (with temperature)\n",
        "    def forward(self, is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels):\n",
        "        is_skill_loss = F.binary_cross_entropy_with_logits(is_skill_logits.float(),\n",
        "                                                           is_skill_labels.float(),\n",
        "                                                           reduction='none')\n",
        "        is_skill_loss *= 1 + ((is_skill_logits > 0.0) & (~is_skill_labels.bool())).float() * self.is_skill_fp_penalty\n",
        "        is_skill_loss = is_skill_loss.mean()\n",
        "        if (skill_id_labels > -1).sum() > 0:\n",
        "            skill_id_loss = F.cross_entropy(skill_id_logits / self.skill_id_temperature, skill_id_labels, ignore_index=-1)\n",
        "        else:\n",
        "            skill_id_loss = torch.tensor(0.0, device=is_skill_logits.device)\n",
        "        return C.IS_SKILL_WEIGHT * is_skill_loss + skill_id_loss, {'is_skill_loss': is_skill_loss.item(), 'skill_id_loss': skill_id_loss.item()}\n",
        "\n",
        "# init criterion\n",
        "criterion = SkillCriterion(is_skill_fp_penalty=C.IS_SKILL_FP_PENALTY, skill_id_temperature=C.SKILL_ID_TEMP_INITIAL).to(C.DEVICE)"
      ],
      "metadata": {
        "id": "kLvh55L89Ey8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "FxdwSjrZ5sr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define metrics (needs vectorizing)\n",
        "class SkillMetrics(nn.Module):\n",
        "    def __init__(self, atp_temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.atp_temperature = atp_temperature\n",
        "\n",
        "    # calculate metrics\n",
        "    def forward(self, is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels, sentences):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # is_skill precision and recall\n",
        "            tp = ((is_skill_logits > 0.0) & is_skill_labels).sum().item()\n",
        "            fp = ((is_skill_logits > 0.0) & ~is_skill_labels).sum().item()\n",
        "            fn = ((is_skill_logits <= 0.0) & is_skill_labels).sum().item()\n",
        "            is_skill_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            is_skill_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            # group entries by unique sentences\n",
        "            sentence_to_indices = {}\n",
        "            for i, sentence in enumerate(sentences.tolist()):\n",
        "                sentence_to_indices.setdefault(sentence, []).append(i)\n",
        "\n",
        "            # init skill_id metrics\n",
        "            mrr_sum, rp5_sum, atp_sum, count = 0, 0, 0, 0\n",
        "\n",
        "            # get probs and ranks\n",
        "            probs = F.softmax(skill_id_logits / self.atp_temperature, dim=1)\n",
        "            ranks = torch.argsort(skill_id_logits, dim=1, descending=True)\n",
        "\n",
        "            # process metrics per unique sentence\n",
        "            for sentence, indices in sentence_to_indices.items():\n",
        "\n",
        "                # aggregate skill labels for the sentence, get softmax probs and sorted ranks\n",
        "                sentence_labels = set(skill_id_labels[i].item() for i in indices if skill_id_labels[i] > -1)\n",
        "\n",
        "                # check for any labels\n",
        "                if len(sentence_labels) == 0:\n",
        "                    continue\n",
        "\n",
        "                # ATP calculation (average true probability)\n",
        "                sentence_atp = probs[indices[0], list(sentence_labels)].sum().item()\n",
        "                atp_sum += sentence_atp / len(sentence_labels)\n",
        "\n",
        "                # MRR calculation\n",
        "                sentence_mrr = 0.0\n",
        "                found_labels = set()\n",
        "                for pos, pred in enumerate(ranks[indices[0]].tolist(), 1):\n",
        "                    if pred in sentence_labels:\n",
        "                        sentence_mrr += 1.0 / (pos - len(found_labels))\n",
        "                        found_labels.add(pred)\n",
        "                        if len(found_labels) == len(sentence_labels):\n",
        "                            break\n",
        "                mrr_sum += sentence_mrr / len(sentence_labels)\n",
        "\n",
        "                # RP@5 calculation\n",
        "                top_k_correct = len(sentence_labels & set(ranks[indices[0], :5].tolist()))\n",
        "                rp5_sum += top_k_correct / min(5, len(sentence_labels))\n",
        "\n",
        "                count += 1\n",
        "\n",
        "        # finalize metrics\n",
        "        if count:\n",
        "            skill_id_atp = atp_sum / count\n",
        "            skill_id_rp5 = rp5_sum / count\n",
        "            skill_id_mrr = mrr_sum / count\n",
        "        else:\n",
        "            skill_id_atp, skill_id_rp5, skill_id_mrr = None, None, None\n",
        "\n",
        "        return {\n",
        "            'is_skill_pre': is_skill_precision,\n",
        "            'is_skill_rec': is_skill_recall,\n",
        "            'skill_id_atp': skill_id_atp,\n",
        "            'skill_id_rp5': skill_id_rp5,\n",
        "            'skill_id_mrr': skill_id_mrr\n",
        "        }\n",
        "\n",
        "# init metrics\n",
        "metrics = SkillMetrics(atp_temperature=C.ATP_TEMPERATURE).to(C.DEVICE)"
      ],
      "metadata": {
        "id": "fU-OnTX73HlC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ydnFzovbmG-"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k9K9SpX6awC9"
      },
      "outputs": [],
      "source": [
        "# set n last layers trainable\n",
        "for idx, layer in enumerate(base_model.encoder.layer):\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = idx >= len(base_model.encoder.layer) - C.N_LAYERS\n",
        "\n",
        "# calculate layers to optimize learning for\n",
        "n_layers = C.N_LAYERS - len(base_model.encoder.layer)\n",
        "\n",
        "# create param groups for optimizer with layer-wise learning rate  (factor applied per layer)\n",
        "param_groups = reversed([\n",
        "    {'params': base_model.encoder.layer[i].parameters(), 'lr': C.LR_INITIAL * C.LR_LAYER_FACTOR**-(i + 1)} for i in range(n_layers, 0)\n",
        "])\n",
        "\n",
        "# adam with weight decay, default settings\n",
        "optimizer = torch.optim.AdamW(param_groups, weight_decay=C.WEIGHT_DECAY_RATE)\n",
        "\n",
        "# mixed precision scaler\n",
        "scaler = torch.amp.GradScaler(str(C.DEVICE))\n",
        "\n",
        "# collection of mixed precision backward pass calls\n",
        "def backward(loss, optimizer, scaler):\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    clip_grad_norm_(embedder.parameters(), max_norm=1.0)\n",
        "    scaler.unscale_(optimizer)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dDtnik0-opH"
      },
      "source": [
        "# Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EfL7605b-nu6"
      },
      "outputs": [],
      "source": [
        "# define module for logging\n",
        "class Logger(nn.Module):\n",
        "\n",
        "    def __init__(self, optimizer, path):\n",
        "        super().__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.path = path\n",
        "\n",
        "    def forward(self, epoch, data):\n",
        "\n",
        "        # init log data fields\n",
        "        log_data = {'datetime': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'epoch': epoch}\n",
        "\n",
        "        # gather log data\n",
        "        log_data |= data\n",
        "\n",
        "        # init log with header\n",
        "        if not os.path.exists(self.path):\n",
        "            with open(self.path, 'w', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(log_data.keys())\n",
        "\n",
        "        # log data for epoch\n",
        "        with open(self.path, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(log_data.values())\n",
        "            f.flush()\n",
        "\n",
        "# initialize logging\n",
        "logger = Logger(optimizer, path=C.PATH('.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "y_5iOXU-zz4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decision variables\n",
        "max_metrics = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "# run through epochs\n",
        "for epoch in range(C.EPOCHS):\n",
        "\n",
        "    # training mode\n",
        "    embedder.train()\n",
        "\n",
        "    # init stats and progress bar\n",
        "    train_stats = {}\n",
        "    model_stats = {}\n",
        "    pbar = tqdm(train_loader, desc=f'Training (epoch {epoch+1}/{C.EPOCHS})', unit='batch')\n",
        "\n",
        "    # train\n",
        "    for num_batch, batch in enumerate(pbar):\n",
        "        with torch.amp.autocast(str(C.DEVICE)):\n",
        "\n",
        "            # send batch to GPU\n",
        "            batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            # generate embeddings\n",
        "            embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "            if C.TRAIN_METHOD == 'mnr':\n",
        "\n",
        "                # predictions\n",
        "                is_skill_logits = predictor(embeddings, include='is_skill', logits=True)\n",
        "                skill_id_logits = F.cosine_similarity(embeddings[:len(embeddings)//4].unsqueeze(1), # 1st 4th\n",
        "                                                      embeddings[len(embeddings)//4:len(embeddings)//2].unsqueeze(0), # 2nd 4th\n",
        "                                                      dim=-1)\n",
        "\n",
        "                # truth\n",
        "                is_skill_labels = batch['id'] > -1\n",
        "                skill_id_labels = torch.arange(len(embeddings)//4, device=C.DEVICE)\n",
        "\n",
        "            else:\n",
        "\n",
        "                # predictions\n",
        "                is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "                # truth\n",
        "                is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "            # run batch\n",
        "            loss, stats = criterion(is_skill_logits, is_skill_labels, skill_id_logits, skill_id_labels)\n",
        "\n",
        "        # backward pass\n",
        "        backward(loss, optimizer, scaler)\n",
        "\n",
        "        # update progress\n",
        "        train_stats = {\n",
        "            'train_loss': [loss.item()] + train_stats.setdefault('loss', []),\n",
        "            'train_is_skill_loss': [stats['is_skill_loss']] + train_stats.setdefault('is_skill_loss', []),\n",
        "            'train_skill_id_loss': [stats['skill_id_loss']] + train_stats.setdefault('skill_id_loss', []),\n",
        "            'lr': [optimizer.param_groups[0]['lr']] + model_stats.setdefault('lr', []),\n",
        "            'patience': [C.PATIENCE - patience_counter],\n",
        "            'temperature': [criterion.skill_id_temperature] + model_stats.setdefault('temperature', []),\n",
        "        }\n",
        "\n",
        "        pbar.set_postfix({k: sum(v) / len(v) for k, v in train_stats.items()})\n",
        "\n",
        "        # decay temperature\n",
        "        criterion.skill_id_temperature = max(criterion.skill_id_temperature * C.SKILL_ID_TEMP_FACTOR, C.SKILL_ID_TEMP)\n",
        "\n",
        "        # warmup lr\n",
        "        for param_group in optimizer.param_groups:\n",
        "            if param_group['lr'] >= C.LR or patience_counter > 0:\n",
        "                break\n",
        "            param_group['lr'] = min(param_group['lr'] * C.LR_WARMUP_FACTOR, C.LR)\n",
        "\n",
        "    # clean up\n",
        "    del batch, embeddings, is_skill_logits, skill_id_logits, is_skill_labels, skill_id_labels, loss, stats\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # update proxy embeddings after training / before validation\n",
        "    predictor.update_embeddings()\n",
        "\n",
        "    # validation mode\n",
        "    embedder.eval()\n",
        "\n",
        "    # init stats and progress bar\n",
        "    val_stats = {}\n",
        "    pbar = tqdm(val_loader, desc=f'Validation (epoch {epoch+1}/{C.EPOCHS})', unit='batch')\n",
        "\n",
        "    # results warehouse\n",
        "    logits_and_labels = dict()\n",
        "\n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "        for num_batch, batch in enumerate(pbar):\n",
        "\n",
        "            # send batch to GPU\n",
        "            batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "            # generate embeddings\n",
        "            embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "            # predictions\n",
        "            is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "            # truth\n",
        "            is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "            # save logits and labels\n",
        "            logits_and_labels.setdefault('is_skill_logits', []).append(is_skill_logits)\n",
        "            logits_and_labels.setdefault('is_skill_labels', []).append(is_skill_labels)\n",
        "            logits_and_labels.setdefault('skill_id_logits', []).append(skill_id_logits)\n",
        "            logits_and_labels.setdefault('skill_id_labels', []).append(skill_id_labels)\n",
        "            logits_and_labels.setdefault('sentence', []).append(batch['sentence'])\n",
        "\n",
        "        # run batch\n",
        "        loss, stats = criterion(*[torch.cat(v) for v in list(logits_and_labels.values())[:4]])\n",
        "        val_metrics = metrics(*[torch.cat(v) for v in logits_and_labels.values()])\n",
        "        stats = {'loss': loss.item()} | stats | val_metrics\n",
        "\n",
        "        # update progress\n",
        "        val_stats = {k: [v] + val_stats.setdefault(k, []) for k, v in stats.items() if v is not None}\n",
        "        print('Validation:', ', '.join([f'{k} = {str(sum(v) / len(v))}' for k, v in val_stats.items()]))\n",
        "\n",
        "    # finalize stats\n",
        "    train_stats = {k: sum(v) / len(v) for k, v in train_stats.items()}\n",
        "    val_stats = {k: sum(v) / len(v) for k, v in val_stats.items()}\n",
        "\n",
        "    # logging\n",
        "    logger(epoch=epoch + 1, data=val_stats | train_stats)\n",
        "\n",
        "    # save or break (if break, load best weights and restore embeddings)\n",
        "    avg_metrics = sum([v for k, v in val_stats.items() if k in val_metrics.keys()])\n",
        "    if avg_metrics > max_metrics:\n",
        "        patience_counter = 0\n",
        "        max_metrics = avg_metrics\n",
        "        torch.save({'embedder_state_dict': embedder.state_dict()}, C.PATH('.pth'))\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= C.PATIENCE:\n",
        "            embedder.load_state_dict(torch.load(C.PATH('.pth'), weights_only=False)['embedder_state_dict'])\n",
        "            predictor.update_embeddings()\n",
        "            break\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] *= C.LR_REDUCE_FACTOR"
      ],
      "metadata": {
        "id": "r_cpM1nHz1G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60b199b-e0e3-4374-b5b3-e98c008cffe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 1/100): 100%|██████████| 10792/10792 [18:20<00:00,  9.81batch/s, train_loss=2.77, train_is_skill_loss=0.688, train_skill_id_loss=2.6, lr=1e-6, patience=3, temperature=0.34]\n",
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:29<00:00,  1.03batch/s]\n",
            "Validation (epoch 1/100): 100%|██████████| 9/9 [00:03<00:00,  2.90batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: loss = 8.310223579406738, is_skill_loss = 0.6870371699333191, skill_id_loss = 8.138463973999023, is_skill_pre = 0.6977152899824253, is_skill_rec = 0.7153153153153153, skill_id_atp = 0.308489127721702, skill_id_rp5 = 0.5512722646310433, skill_id_mrr = 0.4307749980083098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 2/100): 100%|██████████| 10792/10792 [18:20<00:00,  9.80batch/s, train_loss=2.01, train_is_skill_loss=0.681, train_skill_id_loss=1.84, lr=1e-6, patience=3, temperature=0.116]\n",
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:28<00:00,  1.04batch/s]\n",
            "Validation (epoch 2/100): 100%|██████████| 9/9 [00:02<00:00,  3.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: loss = 6.012263298034668, is_skill_loss = 0.6795756816864014, skill_id_loss = 5.842369556427002, is_skill_pre = 0.9210526315789473, is_skill_rec = 0.6936936936936937, skill_id_atp = 0.28346767709384885, skill_id_rp5 = 0.5299618320610686, skill_id_mrr = 0.4082731737982301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 3/100): 100%|██████████| 10792/10792 [18:21<00:00,  9.80batch/s, train_loss=2.34, train_is_skill_loss=0.678, train_skill_id_loss=2.17, lr=1e-6, patience=3, temperature=0.05]\n",
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:28<00:00,  1.04batch/s]\n",
            "Validation (epoch 3/100): 100%|██████████| 9/9 [00:02<00:00,  3.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: loss = 5.294389724731445, is_skill_loss = 0.6746786236763, skill_id_loss = 5.125720024108887, is_skill_pre = 0.8465189873417721, is_skill_rec = 0.963963963963964, skill_id_atp = 0.2918497152561288, skill_id_rp5 = 0.5414122137404579, skill_id_mrr = 0.4304522463582193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 4/100): 100%|██████████| 10792/10792 [18:22<00:00,  9.79batch/s, train_loss=2.09, train_is_skill_loss=0.671, train_skill_id_loss=1.93, lr=1e-6, patience=3, temperature=0.05]\n",
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:28<00:00,  1.04batch/s]\n",
            "Validation (epoch 4/100): 100%|██████████| 9/9 [00:02<00:00,  3.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: loss = 5.413917541503906, is_skill_loss = 0.6703407764434814, skill_id_loss = 5.246332168579102, is_skill_pre = 0.9142367066895368, is_skill_rec = 0.9603603603603603, skill_id_atp = 0.2803674158300044, skill_id_rp5 = 0.5538167938931298, skill_id_mrr = 0.42351996794072916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 5/100): 100%|██████████| 10792/10792 [18:19<00:00,  9.81batch/s, train_loss=2.05, train_is_skill_loss=0.663, train_skill_id_loss=1.88, lr=1e-6, patience=3, temperature=0.05]\n",
            "Updating proxy embeddings: 100%|██████████| 216/216 [03:28<00:00,  1.04batch/s]\n",
            "Validation (epoch 5/100): 100%|██████████| 9/9 [00:02<00:00,  3.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: loss = 5.420886039733887, is_skill_loss = 0.665419340133667, skill_id_loss = 5.254531383514404, is_skill_pre = 0.9492753623188406, is_skill_rec = 0.9441441441441442, skill_id_atp = 0.2749599505463674, skill_id_rp5 = 0.5365139949109414, skill_id_mrr = 0.4150293934712245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (epoch 6/100):  80%|████████  | 8679/10792 [14:46<03:28, 10.14batch/s, train_loss=2.14, train_is_skill_loss=0.667, train_skill_id_loss=1.97, lr=1e-7, patience=2, temperature=0.05]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Q6_uQtvN7a4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init stats and progress bar\n",
        "test_stats = {}\n",
        "pbar = tqdm(test_loader, desc=f'Test', unit='batch')\n",
        "\n",
        "# results warehouse\n",
        "logits_and_labels = dict()\n",
        "\n",
        "# test\n",
        "with torch.no_grad():\n",
        "    for num_batch, batch in enumerate(pbar):\n",
        "\n",
        "        # test mode\n",
        "        embedder.eval()\n",
        "\n",
        "        # send batch to GPU\n",
        "        batch = {k: v.to(C.DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        # generate embeddings\n",
        "        embeddings = embedder(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "        # predictions\n",
        "        is_skill_logits, skill_id_logits = predictor(embeddings, logits=True)\n",
        "\n",
        "        # truth\n",
        "        is_skill_labels, skill_id_labels = batch['id'] > -1, batch['id']\n",
        "\n",
        "        # save logits and labels\n",
        "        logits_and_labels.setdefault('is_skill_logits', []).append(is_skill_logits)\n",
        "        logits_and_labels.setdefault('is_skill_labels', []).append(is_skill_labels)\n",
        "        logits_and_labels.setdefault('skill_id_logits', []).append(skill_id_logits)\n",
        "        logits_and_labels.setdefault('skill_id_labels', []).append(skill_id_labels)\n",
        "        logits_and_labels.setdefault('sentence', []).append(batch['sentence'])\n",
        "        logits_and_labels.setdefault('group', []).append(batch['group'])\n",
        "\n",
        "    # concatenate\n",
        "    logits_and_labels = {k: torch.cat(v) for k, v in logits_and_labels.items()}\n",
        "\n",
        "    # run batch\n",
        "    loss, stats = criterion(*list(logits_and_labels.values())[:4])\n",
        "    stats = {'loss': loss.item()} | stats | metrics(*list(logits_and_labels.values())[:5])\n",
        "\n",
        "    # update progress\n",
        "    test_stats = {k: [v] + test_stats.setdefault(k, []) for k, v in stats.items() if v is not None}\n",
        "    print('Test:', ', '.join([f'{k} = {str(sum(v) / len(v))}' for k, v in test_stats.items()]))\n",
        "\n",
        "# finalize stats\n",
        "test_stats = {k: sum(v) / len(v) for k, v in test_stats.items()}\n",
        "\n",
        "# logging\n",
        "logger(epoch='test', data=test_stats)\n",
        "\n",
        "# group logging\n",
        "for g in logits_and_labels['group'].unique():\n",
        "    loss, stats = criterion(*[v[g == logits_and_labels['group']] for v in list(logits_and_labels.values())[:4]])\n",
        "    stats = {'loss': loss.item()} | stats | metrics(*[v[g == logits_and_labels['group']] for v in list(logits_and_labels.values())[:5]])\n",
        "    logger(epoch=f'group{g}', data=stats)"
      ],
      "metadata": {
        "id": "HG1_fRBXT0SK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeuwlvdfSKtQtd2P40iNvw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}